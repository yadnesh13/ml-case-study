https://www.kaggle.com/code/ayakouks/fake-news-detection




import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import regex as re
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC

real = pd.read_csv("True.csv")
fake = pd.read_csv("Fake.csv")

real['target']=1
fake['target']=0

data = pd.concat([real , fake] , ignore_index=True)
data = data.sample(frac=1, random_state=42).reset_index(drop=True)


display(data.head())
print('-'*40)

print('shape' , data.shape)
print('-'*40)

display(data.dtypes )
print('-'*40)

display(data.isna().sum())
print('\n we have no null values ' )


data.target.value_counts(normalize=True)


rs1 = data[data['target'] == 0].groupby(['subject'], as_index=False).size()
rs1 = rs1.rename(columns={'size': 'count'}).sort_values(by='count', ascending=False)
print(rs1)


rs2 = data[data['target'] == 1].groupby(['subject'], as_index=False).size()
rs2 = rs2.rename(columns={'size': 'count'}).sort_values(by='count', ascending=False)
print(rs2)


subject_distribution = data.groupby(['subject', 'target']).size().unstack(fill_value=0)

# Plotting the bar chart using Matplotlib
subject_distribution.plot(kind='bar', stacked=True)

# Adding labels and title
plt.xlabel('Subject')
plt.ylabel('Number of Articles')
plt.title('Distribution of Subjects Between True and Fake News')

# Show the plot
plt.show()



import matplotlib.pyplot as plt 
import seaborn as sns 

sns.set_style("whitegrid") # Set style for chart
plt.figure(figsize=(6,6)) 
plt.pie(data['subject'].value_counts(),labels=data['subject'].value_counts().index.tolist(), autopct='%1.1f%%')
plt.title('percentage of our subjects')
plt.show()


data.subject=data.subject.replace({'politics':'PoliticsNews','politicsNews':'PoliticsNews'})



import matplotlib.pyplot as plt 
import seaborn as sns 

sns.set_style("whitegrid") # Set style for chart
plt.figure(figsize=(6,6)) 
plt.pie(data['subject'].value_counts(),labels=data['subject'].value_counts().index.tolist(), autopct='%1.1f%%')
plt.title('percentage of our subjects')
plt.show()




# Count the number of articles for each subject
subject_counts = data['subject'].value_counts()

# Get the top subjects with the most news coverage
top_subjects = subject_counts.head(10)  # Adjust the number as needed

# Plot the distribution of subjects
top_subjects.plot(kind='bar', figsize=(10, 6), color='skyblue')

# Add titles and labels
plt.title('Top Subjects with the Most News Coverage')
plt.xlabel('Subject')
plt.ylabel('Number of Articles')

# Show plot
plt.show()



import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import mutual_info_classif

# # Assuming 'data' is your DataFrame with the specified data types

# # Step 1: Tokenize and vectorize text data (title and text columns)
tfidf_vectorizer = TfidfVectorizer(max_features=5000)
title_text_features = tfidf_vectorizer.fit_transform(data['title'] + ' ' + data['text'])

# # Step 2: Encode categorical variables (subject column)
data_encoded = pd.get_dummies(data, columns=['subject'])

# # Step 3: Extract features from date column if needed

# # Step 4: Combine features for information gain calculation
X = pd.concat([data_encoded.drop(columns=['title', 'text', 'date', 'target']), pd.DataFrame(title_text_features.toarray())], axis=1)
y = data['target']

# # Step 5: Calculate information gain for each feature
info_gain = mutual_info_classif(X, y)

# # Create a DataFrame to show the information gain for each feature
info_gain_df = pd.DataFrame({'Feature': X.columns, 'Information Gain': info_gain})
info_gain_df.sort_values(by='Information Gain', ascending=False, inplace=True)

print(info_gain_df)



data['final'] =  data['title'] + " " + data['subject']




import nltk
from nltk.corpus import stopwords
from string import punctuation

# Download the stopwords corpus if it's not already downloaded
try:
    nltk.download('stopwords')
except Exception as e:
    print("Stopwords corpus already downloaded.")

# Load the stopwords
stop = set(stopwords.words('english'))

# Add punctuation to the stopwords set
pnc = list(punctuation)
stop.update(pnc)



stemmer = PorterStemmer()
def stem_text(text):
    final_text = []
    for i in text.split():
        if i.strip().lower() not in stop:
            word = stemmer.stem(i.strip())
            final_text.append(word)
    return " ".join(final_text)



data['final'] = data['final'].apply(stem_text)
data['final'].head(3)



from sklearn.feature_extraction.text import CountVectorizer
X_train,X_test,y_train,y_test = train_test_split(data['final'],data['target'])
cv = CountVectorizer(min_df=0,max_df=1,ngram_range=(1,2))

cv_train = cv.fit_transform(X_train)
cv_test = cv.transform(X_test)

print('Train shape: ',cv_train.shape)
print('Test shape: ',cv_test.shape)


from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score


nb = MultinomialNB()
nb.fit(cv_train, y_train)


pred_nb = nb.predict(cv_test)
score = accuracy_score(y_test, pred_nb)
print("Accuracy Score: ",score)


data['final2'] =  data['text'] + " " + data['title'] + " " + data['subject']
data['final2'] = data['final2'].apply(stem_text)
data['final2'].head(3)



X_train,X_test,y_train,y_test = train_test_split(data['final2'],data['target'])
cv = CountVectorizer(min_df=0,max_df=1,ngram_range=(1,2))

cv_train = cv.fit_transform(X_train)
cv_test = cv.transform(X_test)

print('Train shape: ',cv_train.shape)
print('Test shape: ',cv_test.shape)



nb = MultinomialNB()
nb.fit(cv_train, y_train)

pred_nb = nb.predict(cv_test)
score = accuracy_score(y_test, pred_nb)
print("Accuracy Score: ",score)

